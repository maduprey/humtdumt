{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with HumT scores from prism, uf, and lmsys\n",
    "import pandas as pd\n",
    "\n",
    "ultrafeedback = pd.read_csv('humt_and_sociot_scores/uf_all_1103.csv')\n",
    "\n",
    "lmsys = pd.read_csv(\"humt_and_sociot_scores/lmsys_ab_1103.csv\")\n",
    "\n",
    "prism = pd.read_csv(\"humt_and_sociot_scores/prism_all_1112.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 27028\n",
      "Test size: 793\n"
     ]
    }
   ],
   "source": [
    "# train test split for prism: first drop duplicates, then split\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "unique_conversation_ids = prism['conversation_id'].unique()\n",
    "\n",
    "# Perform 90% train, 10% test split on conversation_id\n",
    "train_ids, test_ids = train_test_split(unique_conversation_ids, test_size=0.1, random_state=42)\n",
    "\n",
    "# Assign train/test labels\n",
    "prism['train'] = prism['conversation_id'].isin(train_ids)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_prism = prism[prism['train']]\n",
    "test_prism = prism[~prism['train']]\n",
    "\n",
    "# Deduplicate user_prompt only in test set\n",
    "test_prism = test_prism.drop_duplicates(subset='user_prompt', keep='first')\n",
    "\n",
    "# Drop the 'train' column before saving final train and test datasets\n",
    "train_prism = train_prism.drop(columns=['train'])\n",
    "test_prism = test_prism.drop(columns=['train'])\n",
    "\n",
    "# Print sizes\n",
    "print(\"Train size:\", len(train_prism))\n",
    "print(\"Test size:\", len(test_prism))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 35087\n",
      "Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "# train test split for ultrafeedback\n",
    "\n",
    "# group by instructions\n",
    "unique_instructions = ultrafeedback['instruction'].unique()\n",
    "\n",
    "# Step 3: Perform 90% train, 10% test split on conversation_id\n",
    "train_ids, test_ids = train_test_split(unique_instructions, test_size=0.1, random_state=42)\n",
    "\n",
    "# Step 4: Assign train/test labels\n",
    "ultrafeedback['train'] = ultrafeedback['instruction'].isin(train_ids)\n",
    "\n",
    "# Step 5: Split into train and test sets\n",
    "train_uf = ultrafeedback[ultrafeedback['train']]\n",
    "test_uf = ultrafeedback[~ultrafeedback['train']]\n",
    "\n",
    "# Step 6: Deduplicate user_prompt only in test set\n",
    "test_uf = test_uf.drop_duplicates(subset='instruction', keep='first')\n",
    "\n",
    "# Drop the 'train' column before saving final train and test datasets\n",
    "train_uf = train_uf.drop(columns=['train'])\n",
    "test_uf = test_uf.drop(columns=['train'])\n",
    "\n",
    "# Print sizes\n",
    "print(\"Train size:\", len(train_uf))\n",
    "print(\"Test size:\", len(test_uf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17373\n",
      "train size: 15944\n",
      "test size: 1772\n"
     ]
    }
   ],
   "source": [
    "# filter unsafe prompts, then train test split for lmsys\n",
    "# each row is a pair so we can just split the rows\n",
    "\n",
    "import json\n",
    "lmsys= lmsys.drop_duplicates(subset='prompt', keep='first')\n",
    "lmsys\n",
    "\n",
    "# Define a function to check if a response is unsafe\n",
    "def is_unsafe(row):\n",
    "    openai_str = str(row['openai_moderation'])  # Ensure it's a string\n",
    "    toxic_str = str(row['toxic_chat_tag'])      # Ensure it's a string\n",
    "\n",
    "    # Check if \"flagged\": true is in either column\n",
    "    if \"'flagged': True\" in openai_str or \"'flagged': True\" in toxic_str:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Apply the filter\n",
    "df_filtered = lmsys[~lmsys.apply(is_unsafe, axis=1)]\n",
    "print(len(df_filtered))\n",
    "train_lmsys, test_lmsys = train_test_split(lmsys, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"train size:\", len(train_lmsys))\n",
    "print(\"test size:\", len(test_lmsys))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct preference training set \n",
    "\n",
    "# make sure difference is > threshold and the one with lower score is actually more preferred\n",
    "\n",
    "# prism\n",
    "import itertools\n",
    "\n",
    "def create_comparison_dataset(dataset, threshold):\n",
    "    prefs = []\n",
    "    for interaction_id, group in dataset.groupby('interaction_id'):\n",
    "        # Get the indices of all items in the current group\n",
    "        group_indices = group.index.tolist()\n",
    "        \n",
    "        # Generate all possible pairs within the group\n",
    "        pairs = itertools.combinations(group_indices, 2)\n",
    "        \n",
    "        # Iterate over each pair\n",
    "        for idx_a, idx_b in pairs:\n",
    "            a = group.loc[idx_a]\n",
    "            b = group.loc[idx_b]\n",
    "        \n",
    "        \n",
    "            if (a['human_ai_pronouns_ratio_gpt2_model_response_300'] - b['human_ai_pronouns_ratio_gpt2_model_response_300'] > threshold) and (b['score'] > a['score']):\n",
    "                # Append the preference where 'a' is chosen over 'b'\n",
    "                # apply chat template to the prompt to ensure it is in the right format\n",
    "                prefs.append({\n",
    "                    'prompt': a['user_prompt'],\n",
    "                    'chosen': b['model_response'],\n",
    "                    'rejected': a['model_response'],\n",
    "                    'source': 'prism'\n",
    "                })\n",
    "\n",
    "            elif (b['human_ai_pronouns_ratio_gpt2_model_response_300'] - a['human_ai_pronouns_ratio_gpt2_model_response_300'] > threshold) and (a['score'] > b['score']):\n",
    "                # Append the preference where 'a' is chosen over 'b'\n",
    "                # apply chat template to the prompt to ensure it is in the right format\n",
    "                prefs.append({\n",
    "                    'prompt': a['user_prompt'],\n",
    "                    'chosen': a['model_response'],\n",
    "                    'rejected': b['model_response'],\n",
    "                    'source': 'prism'\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "    return prefs\n",
    "  \n",
    "\n",
    "\n",
    "# uf\n",
    "\n",
    "\n",
    "def create_comparison_dataset_uf(dataset, threshold):\n",
    "    prefs = []\n",
    "    # for the same interaction_id (same user_prompt), compare all the pairs of chosen and rejected\n",
    "    # Group the data by 'interaction_id'\n",
    "    for interaction_id, group in dataset.groupby('instruction'):\n",
    "        # Get the indices of all items in the current group\n",
    "        group_indices = group.index.tolist()\n",
    "        \n",
    "        # Generate all possible pairs within the group\n",
    "        pairs = itertools.combinations(group_indices, 2)\n",
    "        \n",
    "        # Iterate over each pair\n",
    "        for idx_a, idx_b in pairs:\n",
    "            a = group.loc[idx_a]\n",
    "            b = group.loc[idx_b]\n",
    "            \n",
    "            # Check the heuristic conditions for both directions\n",
    "            if (a['human_ai_pronouns_ratio_gpt2_model_response_300'] - b['human_ai_pronouns_ratio_gpt2_model_response_300'] > threshold) and (b['overall_score'] > a['overall_score']):\n",
    "                # Append the preference where 'a' is chosen over 'b'\n",
    "                prefs.append({\n",
    "                    'prompt': a['instruction'],\n",
    "                    'chosen': b['model_response'],\n",
    "                    'rejected': a['model_response'],\n",
    "                    'source': 'uf'\n",
    "                })\n",
    "            elif (b['human_ai_pronouns_ratio_gpt2_model_response_300'] - a['human_ai_pronouns_ratio_gpt2_model_response_300'] > threshold) and (a['overall_score'] > b['overall_score']):\n",
    "                # Append the preference where 'b' is chosen over 'a'\n",
    "                prefs.append({\n",
    "                    'prompt': a['instruction'],\n",
    "                    'chosen': a['model_response'],\n",
    "                    'rejected': b['model_response'],\n",
    "                    'source': 'uf'\n",
    "                })\n",
    "    return prefs\n",
    "  \n",
    "\n",
    "\n",
    "# lmsys\n",
    "\n",
    "import ast\n",
    "def create_comparison_dataset_lmsys(dataset, threshold):\n",
    "    prefs = []\n",
    "#     row['conversation']\n",
    "    # for the same interaction_id (same user_prompt), compare all the pairs of chosen and rejected\n",
    "    # Group the data by 'interaction_id'\n",
    "    for _, row in dataset.iterrows():\n",
    "#         print(row['human_ai_pronouns_ratio_gpt2_model_a_response_300'])\n",
    "#         print(row['human_ai_pronouns_ratio_gpt2_model_b_response_300'])\n",
    "#         print(row['conversation_a'][0])\n",
    "        if (row['human_ai_pronouns_ratio_gpt2_model_a_response_300'] - row['human_ai_pronouns_ratio_gpt2_model_b_response_300'] > threshold) and (row['winner'] == 'model_b'):\n",
    "            prefs.append({\n",
    "                    'prompt': row[\"prompt\"], # need to make sure the conversation columns are lists, not strings!\n",
    "                    'chosen': row['model_b_response'],\n",
    "                    'rejected': row['model_a_response'],\n",
    "                    'source': 'lmsys'\n",
    "                })\n",
    "        elif (row['human_ai_pronouns_ratio_gpt2_model_b_response_300'] - row['human_ai_pronouns_ratio_gpt2_model_a_response_300'] > threshold) and (row['winner'] == 'model_a'):\n",
    "            prefs.append({\n",
    "                    'prompt': row[\"prompt\"],#[0]['content'],\n",
    "                   'chosen': row['model_a_response'],\n",
    "                    'rejected': row['model_b_response'],\n",
    "                    'source': 'lmsys'\n",
    "                })\n",
    "    return prefs\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thr = 0 # threshold size t\n",
    "size = 500  #dataset size n\n",
    "PREFERENCE_DATA = create_comparison_dataset(train_prism, thr)\n",
    "PREFERENCE_DATA_UF = create_comparison_dataset_uf(train_uf, thr)\n",
    "PREFERENCE_DATA_LMSYS = create_comparison_dataset_lmsys(train_lmsys, thr)\n",
    "training_data = list(pd.Series(PREFERENCE_DATA + PREFERENCE_DATA_UF + PREFERENCE_DATA_LMSYS).sample(500))\n",
    "len(training_data)\n",
    "th = str(thr*100)\n",
    "with open(f\"training_{th}_{size}.json\", \"w\") as json_file:\n",
    "    json.dump(training_data, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_{DPO_R} - create dataset for baseline with regular preferences, not accounting for HuMT\n",
    "\n",
    "import itertools\n",
    "# prism\n",
    "def create_comparison_dataset_r(dataset, threshold):\n",
    "    prefs = []\n",
    "    for interaction_id, group in dataset.groupby('interaction_id'):\n",
    "        # Get the indices of all items in the current group\n",
    "        group_indices = group.index.tolist()\n",
    "        \n",
    "        # Generate all possible pairs within the group\n",
    "        pairs = itertools.combinations(group_indices, 2)\n",
    "        \n",
    "        # Iterate over each pair\n",
    "        for idx_a, idx_b in pairs:\n",
    "            a = group.loc[idx_a]\n",
    "            b = group.loc[idx_b]\n",
    "            if (b['score'] > a['score']):\n",
    "                # Append the preference where 'a' is chosen over 'b'\n",
    "                # apply chat template to the prompt to ensure it is in the right format\n",
    "                prefs.append({\n",
    "                    'prompt': a['user_prompt'],\n",
    "                    'chosen': b['model_response'],\n",
    "                    'rejected': a['model_response'],\n",
    "                    'source': 'prism'\n",
    "                })\n",
    "\n",
    "            elif (a['score'] > b['score']):\n",
    "                # Append the preference where 'a' is chosen over 'b'\n",
    "                # apply chat template to the prompt to ensure it is in the right format\n",
    "                prefs.append({\n",
    "                    'prompt': a['user_prompt'],\n",
    "                    'chosen': a['model_response'],\n",
    "                    'rejected': b['model_response'],\n",
    "                    'source': 'prism'\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "    return prefs\n",
    "  \n",
    "\n",
    "\n",
    "# uf\n",
    "\n",
    "\n",
    "def create_comparison_dataset_uf_r(dataset, threshold):\n",
    "    prefs = []\n",
    "    # for the same interaction_id (same user_prompt), compare all the pairs of chosen and rejected\n",
    "    # Group the data by 'interaction_id'\n",
    "    for interaction_id, group in dataset.groupby('instruction'):\n",
    "        # Get the indices of all items in the current group\n",
    "        group_indices = group.index.tolist()\n",
    "        \n",
    "        # Generate all possible pairs within the group\n",
    "        pairs = itertools.combinations(group_indices, 2)\n",
    "        \n",
    "        # Iterate over each pair\n",
    "        for idx_a, idx_b in pairs:\n",
    "            a = group.loc[idx_a]\n",
    "            b = group.loc[idx_b]\n",
    "            \n",
    "            # Check the heuristic conditions for both directions\n",
    "            if  (b['overall_score'] > a['overall_score']):\n",
    "                # Append the preference where 'a' is chosen over 'b'\n",
    "                prefs.append({\n",
    "                    'prompt': a['instruction'],\n",
    "                    'chosen': b['model_response'],\n",
    "                    'rejected': a['model_response'],\n",
    "                    'source': 'uf'\n",
    "                })\n",
    "            elif (a['overall_score'] > b['overall_score']):\n",
    "                # Append the preference where 'b' is chosen over 'a'\n",
    "                prefs.append({\n",
    "                    'prompt': a['instruction'],\n",
    "                    'chosen': a['model_response'],\n",
    "                    'rejected': b['model_response'],\n",
    "                    'source': 'uf'\n",
    "                })\n",
    "    return prefs\n",
    "  \n",
    "\n",
    "\n",
    "# lmsys\n",
    "\n",
    "import ast\n",
    "def create_comparison_dataset_lmsys_r(dataset, threshold):\n",
    "    prefs = []\n",
    "#     row['conversation']\n",
    "    # for the same interaction_id (same user_prompt), compare all the pairs of chosen and rejected\n",
    "    # Group the data by 'interaction_id'\n",
    "    for _, row in dataset.iterrows():\n",
    "        if (row['winner'] == 'model_b'):\n",
    "            prefs.append({\n",
    "                    'prompt': row[\"prompt\"], # need to make sure the conversation columns are lists, not strings!\n",
    "                    'chosen': row['model_b_response'],\n",
    "                    'rejected': row['model_a_response'],\n",
    "                    'source': 'lmsys'\n",
    "                })\n",
    "        elif (row['winner'] == 'model_a'):\n",
    "            prefs.append({\n",
    "                    'prompt': row[\"prompt\"],#[0]['content'],\n",
    "                   'chosen': row['model_a_response'],\n",
    "                    'rejected': row['model_b_response'],\n",
    "                    'source': 'lmsys'\n",
    "                })\n",
    "    return prefs\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PREFERENCE_DATA = create_comparison_dataset_r(train_prism, thr)\n",
    "PREFERENCE_DATA_UF = create_comparison_dataset_uf_r(train_uf, thr)\n",
    "\n",
    "PREFERENCE_DATA_LMSYS = create_comparison_dataset_lmsys_r(train_lmsys, thr)\n",
    "training_data = list(pd.Series(PREFERENCE_DATA + PREFERENCE_DATA_UF + PREFERENCE_DATA_LMSYS).sample(size))\n",
    "len(training_data)\n",
    "th = str(thr*100)\n",
    "with open(f\"training_random_{size}.json\", \"w\") as json_file:\n",
    "    json.dump(training_data, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What should I do if I feel like I am so unhapp...</td>\n",
       "      <td>prism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I would like to write a poem about my white cat.</td>\n",
       "      <td>prism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I start learning how to speak Japanese...</td>\n",
       "      <td>prism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Should people lose their jobs over not getting...</td>\n",
       "      <td>prism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel a connection to my team at work that ma...</td>\n",
       "      <td>prism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3560</th>\n",
       "      <td>Hi! What is Ai Alignment?</td>\n",
       "      <td>lmsys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3561</th>\n",
       "      <td>J'ai un casse-tête que je n'arrive pas à résoudre</td>\n",
       "      <td>lmsys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>How big is the Eiffeltower?</td>\n",
       "      <td>lmsys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>in fortnite what is the different between blue...</td>\n",
       "      <td>lmsys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>Give a list of videogames like the film The Si...</td>\n",
       "      <td>lmsys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3565 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt source\n",
       "0     What should I do if I feel like I am so unhapp...  prism\n",
       "1      I would like to write a poem about my white cat.  prism\n",
       "2     How can I start learning how to speak Japanese...  prism\n",
       "3     Should people lose their jobs over not getting...  prism\n",
       "4     I feel a connection to my team at work that ma...  prism\n",
       "...                                                 ...    ...\n",
       "3560                          Hi! What is Ai Alignment?  lmsys\n",
       "3561  J'ai un casse-tête que je n'arrive pas à résoudre  lmsys\n",
       "3562                        How big is the Eiffeltower?  lmsys\n",
       "3563  in fortnite what is the different between blue...  lmsys\n",
       "3564  Give a list of videogames like the film The Si...  lmsys\n",
       "\n",
       "[3565 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge test sets and only keep the prompt and add a new column for the source\n",
    "\n",
    "# Add a new 'source' column to each test set\n",
    "test_prism['source'] = 'prism'\n",
    "test_uf['source'] = 'uf'\n",
    "test_lmsys['source'] = 'lmsys'\n",
    "\n",
    "# Standardize column names to 'prompt' for merging\n",
    "test_prism = test_prism.rename(columns={'user_prompt': 'prompt'})\n",
    "test_uf = test_uf.rename(columns={'instruction': 'prompt'})\n",
    "test_lmsys = test_lmsys[['prompt', 'source']]  # Already has the correct name\n",
    "\n",
    "# Merge all test sets\n",
    "test = pd.concat([test_prism[['prompt', 'source']], \n",
    "                  test_uf[['prompt', 'source']], \n",
    "                  test_lmsys[['prompt', 'source']]], \n",
    "                 axis=0, ignore_index=True)\n",
    "\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"test_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "lmsys    1772\n",
       "uf       1000\n",
       "prism     793\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.source.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myra",
   "language": "python",
   "name": "myra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
